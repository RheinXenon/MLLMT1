"""
Lingshu-7B æ¨¡å‹æµ‹è¯•è„šæœ¬
æ”¯æŒå¤šç§åŠ è½½æ–¹å¼ï¼šæ ‡å‡†åŠ è½½ã€é‡åŒ–åŠ è½½ã€CPUåŠ è½½
"""

import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
import os
import sys

def check_gpu():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"âœ… GPU: {gpu_name}")
        print(f"âœ… æ˜¾å­˜: {gpu_memory:.2f} GB")
        return True, gpu_memory
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        return False, 0

def load_model(model_path, mode="auto"):
    """
    åŠ è½½æ¨¡å‹
    mode: auto, standard, 4bit, 8bit, cpu
    """
    print(f"\nåŠ è½½æ¨¡å‹ä¸­... (æ¨¡å¼: {mode})")
    print("-" * 60)
    
    # åŠ è½½å¤„ç†å™¨ï¼ˆåŒ…å«åˆ†è¯å™¨å’Œå›¾åƒå¤„ç†å™¨ï¼‰
    print("ğŸ“– åŠ è½½å¤„ç†å™¨...")
    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
    
    # æ ¹æ®æ¨¡å¼åŠ è½½æ¨¡å‹
    print(f"ğŸ”§ åŠ è½½æ¨¡å‹ ({mode} æ¨¡å¼)...")
    
    if mode == "4bit":
        # 4-bit é‡åŒ–åŠ è½½ï¼ˆæ˜¾å­˜å ç”¨æœ€å°ï¼‰
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True
        )
        
    elif mode == "8bit":
        # 8-bit é‡åŒ–åŠ è½½ï¼ˆæ˜¾å­˜å ç”¨è¾ƒå°ï¼Œç²¾åº¦æ¯”4bité«˜ï¼‰
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            load_in_8bit=True,
            device_map="auto",
            trust_remote_code=True
        )
        
    elif mode == "cpu":
        # CPU æ¨¡å¼ï¼ˆæ— éœ€GPUï¼‰
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
    else:
        # æ ‡å‡†åŠ è½½ï¼ˆéœ€è¦è¶³å¤Ÿæ˜¾å­˜ï¼‰
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model, processor

def test_inference(model, processor):
    """æµ‹è¯•æ¨¡å‹æ¨ç†"""
    print("\n" + "=" * 60)
    print("å¼€å§‹æ¨¡å‹æ¨ç†æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•é—®é¢˜åˆ—è¡¨
    test_prompts = [
        "è¯·ä»‹ç»ä¸€ä¸‹é«˜è¡€å‹çš„ç—‡çŠ¶å’Œæ²»ç–—æ–¹æ³•ã€‚",
        "æ„Ÿå†’å’Œæµæ„Ÿæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "ç³–å°¿ç—…æ‚£è€…åº”è¯¥æ³¨æ„å“ªäº›é¥®é£Ÿé—®é¢˜ï¼Ÿ"
    ]
    
    print("\nè¯·é€‰æ‹©æµ‹è¯•æ–¹å¼ï¼š")
    print("1. ä½¿ç”¨é¢„è®¾é—®é¢˜æµ‹è¯•")
    print("2. è¾“å…¥è‡ªå®šä¹‰é—®é¢˜")
    
    choice = input("\nè¯·é€‰æ‹© (1/2): ").strip()
    
    if choice == "2":
        custom_prompt = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
        if custom_prompt:
            test_prompts = [custom_prompt]
    
    # æ‰§è¡Œæ¨ç†
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n{'=' * 60}")
        print(f"æµ‹è¯• {i}/{len(test_prompts)}")
        print(f"{'=' * 60}")
        print(f"â“ é—®é¢˜: {prompt}")
        print(f"\nğŸ’­ ç”Ÿæˆä¸­...")
        
        try:
            # æ„å»ºæ¶ˆæ¯æ ¼å¼ï¼ˆçº¯æ–‡æœ¬æ¨¡å¼ï¼‰
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # å¤„ç†è¾“å…¥ï¼ˆä»…æ–‡æœ¬ï¼Œæ— å›¾åƒï¼‰
            inputs = processor(
                text=[text],
                images=None,
                videos=None,
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to(model.device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    repetition_penalty=1.1
                )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            # è§£ç è¾“å‡º
            response = processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )[0]
            
            print(f"\nâœ… å›ç­”:")
            print("-" * 60)
            print(response)
            print("-" * 60)
            
        except Exception as e:
            print(f"\nâŒ æ¨ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("Lingshu-7B æ¨¡å‹æµ‹è¯•å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = "../models/Lingshu-7B"
    
    if not os.path.exists(model_path):
        print(f"\nâŒ é”™è¯¯: æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        print(f"è·¯å¾„: {os.path.abspath(model_path)}")
        print("\nè¯·å…ˆè¿è¡Œ '1_ä¸‹è½½æ¨¡å‹.py' ä¸‹è½½æ¨¡å‹")
        return
    
    print(f"\nâœ… æ‰¾åˆ°æ¨¡å‹: {os.path.abspath(model_path)}")
    
    # æ£€æŸ¥GPU
    has_gpu, gpu_memory = check_gpu()
    
    # é€‰æ‹©åŠ è½½æ¨¡å¼
    print("\n" + "=" * 60)
    print("è¯·é€‰æ‹©åŠ è½½æ¨¡å¼ï¼š")
    print("-" * 60)
    
    if has_gpu:
        print("1. æ ‡å‡†æ¨¡å¼ (FP16) - éœ€è¦çº¦ 14GB æ˜¾å­˜")
        print("2. 8-bit é‡åŒ– - éœ€è¦çº¦ 7-8GB æ˜¾å­˜ (æ¨è)")
        print("3. 4-bit é‡åŒ– - éœ€è¦çº¦ 4-5GB æ˜¾å­˜")
        print("4. CPU æ¨¡å¼ - æ— éœ€GPUï¼Œä½†é€Ÿåº¦å¾ˆæ…¢")
        
        if gpu_memory < 14:
            print(f"\nğŸ’¡ å»ºè®®: æ‚¨çš„æ˜¾å­˜ä¸º {gpu_memory:.2f}GBï¼Œæ¨èä½¿ç”¨ 8-bit æˆ– 4-bit æ¨¡å¼")
            default_choice = "2"
        else:
            default_choice = "1"
    else:
        print("1. CPU æ¨¡å¼ - æ— éœ€GPUï¼ˆå”¯ä¸€å¯ç”¨é€‰é¡¹ï¼‰")
        default_choice = "1"
    
    choice = input(f"\nè¯·é€‰æ‹© (é»˜è®¤ {default_choice}): ").strip() or default_choice
    
    # æ˜ å°„é€‰æ‹©åˆ°æ¨¡å¼
    mode_map = {
        "1": "standard" if has_gpu else "cpu",
        "2": "8bit",
        "3": "4bit",
        "4": "cpu"
    }
    
    mode = mode_map.get(choice, "auto")
    
    try:
        # åŠ è½½æ¨¡å‹
        model, processor = load_model(model_path, mode)
        
        # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
        if hasattr(model, 'hf_device_map'):
            print(f"\nğŸ“Š æ¨¡å‹è®¾å¤‡åˆ†é…: {model.hf_device_map}")
        
        # æµ‹è¯•æ¨ç†
        test_inference(model, processor)
        
        print("\n" + "=" * 60)
        print("âœ… æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ä¸­æ–­")
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š")
        print("1. å¦‚æœæ˜¯æ˜¾å­˜ä¸è¶³é”™è¯¯ï¼Œè¯·å°è¯•æ›´ä½çš„é‡åŒ–æ¨¡å¼")
        print("2. å¦‚æœæ˜¯ä¾èµ–åŒ…é”™è¯¯ï¼Œè¯·è¿è¡Œ:")
        print("   pip install accelerate bitsandbytes")
        print("3. æŸ¥çœ‹å®Œæ•´é”™è¯¯ä¿¡æ¯:")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

