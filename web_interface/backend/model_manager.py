"""
æ¨¡å‹ç®¡ç†å™¨ - è´Ÿè´£åŠ è½½å’Œç®¡ç†Lingshu-7Bæ¨¡å‹
"""

import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig, TextIteratorStreamer
from qwen_vl_utils import process_vision_info
import logging
from typing import Optional, Dict, Any, List, Generator
import gc
from threading import Thread

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ç±»"""
    
    def __init__(self, model_path: str, quantization: str = "4bit"):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            quantization: é‡åŒ–æ¨¡å¼ (4bit, 8bit, standard, cpu)
        """
        self.model_path = model_path
        self.quantization = quantization
        self.model = None
        self.processor = None
        self.device = None
        
    def check_gpu(self) -> tuple[bool, float]:
        """æ£€æŸ¥GPUå¯ç”¨æ€§"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"âœ… GPU: {gpu_name}")
            logger.info(f"âœ… æ˜¾å­˜: {gpu_memory:.2f} GB")
            return True, gpu_memory
        else:
            logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            return False, 0
    
    def load_model(self) -> bool:
        """
        åŠ è½½æ¨¡å‹
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ”§ å¼€å§‹åŠ è½½æ¨¡å‹ (é‡åŒ–æ¨¡å¼: {self.quantization})...")
            
            # åŠ è½½å¤„ç†å™¨
            logger.info("ğŸ“– åŠ è½½å¤„ç†å™¨...")
            self.processor = AutoProcessor.from_pretrained(
                self.model_path, 
                trust_remote_code=True
            )
            
            # æ ¹æ®é‡åŒ–æ¨¡å¼åŠ è½½æ¨¡å‹
            if self.quantization == "4bit":
                logger.info("ä½¿ç”¨4-bité‡åŒ–æ¨¡å¼")
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    quantization_config=quantization_config,
                    device_map="auto",
                    trust_remote_code=True
                )
                
            elif self.quantization == "8bit":
                logger.info("ä½¿ç”¨8-bité‡åŒ–æ¨¡å¼")
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    load_in_8bit=True,
                    device_map="auto",
                    trust_remote_code=True
                )
                
            elif self.quantization == "cpu":
                logger.info("ä½¿ç”¨CPUæ¨¡å¼")
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                
            else:
                logger.info("ä½¿ç”¨æ ‡å‡†æ¨¡å¼")
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                    trust_remote_code=True
                )
            
            self.device = self.model.device
            logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ! è®¾å¤‡: {self.device}")
            
            # æ˜¾ç¤ºè®¾å¤‡åˆ†é…ä¿¡æ¯
            if hasattr(self.model, 'hf_device_map'):
                logger.info(f"ğŸ“Š è®¾å¤‡æ˜ å°„: {self.model.hf_device_map}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_response(
        self, 
        prompt: str, 
        image_path: Optional[str] = None,
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå›å¤ï¼ˆä¸å¸¦å†å²è®°å½•ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            image_path: å›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            generation_config: ç”Ÿæˆé…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒ…å«ç”Ÿæˆç»“æœçš„å­—å…¸
        """
        return self.generate_response_with_history(
            prompt=prompt,
            image_path=image_path,
            history=[],
            generation_config=generation_config
        )
    
    def generate_response_with_history(
        self,
        prompt: str,
        image_path: Optional[str] = None,
        history: Optional[List[Dict[str, Any]]] = None,
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå›å¤ï¼ˆæ”¯æŒå¯¹è¯å†å²ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            image_path: å›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            generation_config: ç”Ÿæˆé…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒ…å«ç”Ÿæˆç»“æœçš„å­—å…¸
        """
        if self.model is None or self.processor is None:
            return {
                "success": False,
                "error": "æ¨¡å‹æœªåŠ è½½"
            }
        
        try:
            if history is None:
                history = []
            
            logger.info(f"ğŸ¤” ç”Ÿæˆå›å¤: {prompt[:50]}... (å†å²æ¶ˆæ¯æ•°: {len(history)})")
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å†å²å¯¹è¯
            messages = []
            
            # æ·»åŠ å†å²æ¶ˆæ¯
            for hist in history:
                role = hist.get('role')
                content = hist.get('content')
                
                if role and content:
                    # å†å²æ¶ˆæ¯åªåŒ…å«æ–‡æœ¬ï¼ˆå›¾ç‰‡ä¸é‡å¤å‘é€ï¼‰
                    messages.append({
                        "role": role,
                        "content": [{"type": "text", "text": content}]
                    })
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            current_content = []
            if image_path:
                current_content.append({
                    "type": "image",
                    "image": image_path
                })
                logger.info(f"ğŸ–¼ï¸ åŒ…å«å›¾ç‰‡: {image_path}")
            
            current_content.append({"type": "text", "text": prompt})
            
            messages.append({
                "role": "user",
                "content": current_content
            })
            
            logger.info(f"ğŸ“ æ¶ˆæ¯æ€»æ•°: {len(messages)}")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # å¤„ç†è§†è§‰ä¿¡æ¯ï¼ˆåªå¤„ç†å½“å‰æ¶ˆæ¯ï¼‰
            image_inputs = None
            video_inputs = None
            if image_path:
                # åªå¤„ç†å½“å‰çš„å›¾ç‰‡æ¶ˆæ¯
                current_messages = [messages[-1]]
                image_inputs, video_inputs = process_vision_info(current_messages)
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to(self.model.device)
            
            # é»˜è®¤ç”Ÿæˆé…ç½®
            default_config = {
                "max_new_tokens": 512,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "repetition_penalty": 1.1
            }
            
            # åˆå¹¶ç”¨æˆ·é…ç½®
            if generation_config:
                default_config.update(generation_config)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    **default_config
                )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            # è§£ç è¾“å‡º
            response = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )[0]
            
            logger.info(f"âœ… ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(response)}")
            
            return {
                "success": True,
                "response": response,
                "has_image": image_path is not None
            }
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": str(e)
            }
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾å†…å­˜"""
        try:
            if self.model is not None:
                del self.model
                self.model = None
            if self.processor is not None:
                del self.processor
                self.processor = None
            
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            logger.info("âœ… æ¨¡å‹å·²å¸è½½")
            return True
        except Exception as e:
            logger.error(f"âŒ å¸è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def is_loaded(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return self.model is not None and self.processor is not None
    
    def generate_response_stream(
        self,
        prompt: str,
        image_path: Optional[str] = None,
        history: Optional[List[Dict[str, Any]]] = None,
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Generator[str, None, None]:
        """
        ç”Ÿæˆå›å¤ï¼ˆæµå¼è¾“å‡ºï¼Œæ”¯æŒå¯¹è¯å†å²ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            image_path: å›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            generation_config: ç”Ÿæˆé…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        if self.model is None or self.processor is None:
            yield "[é”™è¯¯] æ¨¡å‹æœªåŠ è½½"
            return
        
        try:
            if history is None:
                history = []
            
            logger.info(f"ğŸ¤” æµå¼ç”Ÿæˆå›å¤: {prompt[:50]}... (å†å²æ¶ˆæ¯æ•°: {len(history)})")
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å†å²å¯¹è¯
            messages = []
            
            # æ·»åŠ å†å²æ¶ˆæ¯
            for hist in history:
                role = hist.get('role')
                content = hist.get('content')
                
                if role and content:
                    # å†å²æ¶ˆæ¯åªåŒ…å«æ–‡æœ¬ï¼ˆå›¾ç‰‡ä¸é‡å¤å‘é€ï¼‰
                    messages.append({
                        "role": role,
                        "content": [{"type": "text", "text": content}]
                    })
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            current_content = []
            if image_path:
                current_content.append({
                    "type": "image",
                    "image": image_path
                })
                logger.info(f"ğŸ–¼ï¸ åŒ…å«å›¾ç‰‡: {image_path}")
            
            current_content.append({"type": "text", "text": prompt})
            
            messages.append({
                "role": "user",
                "content": current_content
            })
            
            logger.info(f"ğŸ“ æ¶ˆæ¯æ€»æ•°: {len(messages)}")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # å¤„ç†è§†è§‰ä¿¡æ¯ï¼ˆåªå¤„ç†å½“å‰æ¶ˆæ¯ï¼‰
            image_inputs = None
            video_inputs = None
            if image_path:
                # åªå¤„ç†å½“å‰çš„å›¾ç‰‡æ¶ˆæ¯
                current_messages = [messages[-1]]
                image_inputs, video_inputs = process_vision_info(current_messages)
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to(self.model.device)
            
            # é»˜è®¤ç”Ÿæˆé…ç½®
            default_config = {
                "max_new_tokens": 512,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "repetition_penalty": 1.1
            }
            
            # åˆå¹¶ç”¨æˆ·é…ç½®
            if generation_config:
                default_config.update(generation_config)
            
            # åˆ›å»ºæµå¼è¾“å‡ºå™¨
            streamer = TextIteratorStreamer(
                self.processor.tokenizer,
                skip_prompt=True,
                skip_special_tokens=True
            )
            
            # æ·»åŠ streameråˆ°ç”Ÿæˆé…ç½®
            generation_kwargs = {
                **inputs,
                **default_config,
                "streamer": streamer
            }
            
            # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­ç”Ÿæˆ
            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()
            
            # æµå¼è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
            for text_chunk in streamer:
                yield text_chunk
            
            thread.join()
            
            logger.info("âœ… æµå¼ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            yield f"[é”™è¯¯] {str(e)}"

