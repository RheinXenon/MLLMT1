"""
æ¨¡å‹ç®¡ç†å™¨ - è´Ÿè´£åŠ è½½å’Œç®¡ç†Lingshu-7Bæ¨¡å‹
"""

import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig, TextIteratorStreamer
from qwen_vl_utils import process_vision_info
import logging
from typing import Optional, Dict, Any, List, Generator
import gc
from threading import Thread
from PIL import Image
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ç±»"""
    
    def __init__(self, model_path: str, quantization: str = "4bit", max_pixels: int = 1003520):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            quantization: é‡åŒ–æ¨¡å¼ (4bit, 8bit, standard, cpu)
            max_pixels: æœ€å¤§åƒç´ æ•°ï¼Œé»˜è®¤1003520(çº¦100ä¸‡åƒç´ ï¼Œé€‚åˆ8GBæ˜¾å­˜)
        """
        self.model_path = model_path
        self.quantization = quantization
        self.max_pixels = max_pixels
        self.model = None
        self.processor = None
        self.device = None
        
    def check_gpu(self) -> tuple[bool, float]:
        """æ£€æŸ¥GPUå¯ç”¨æ€§"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"âœ… GPU: {gpu_name}")
            logger.info(f"âœ… æ˜¾å­˜: {gpu_memory:.2f} GB")
            return True, gpu_memory
        else:
            logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            return False, 0
    
    def load_model(self) -> bool:
        """
        åŠ è½½æ¨¡å‹
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ”§ å¼€å§‹åŠ è½½æ¨¡å‹ (é‡åŒ–æ¨¡å¼: {self.quantization})...")
            
            # åŠ è½½å¤„ç†å™¨
            logger.info("ğŸ“– åŠ è½½å¤„ç†å™¨...")
            self.processor = AutoProcessor.from_pretrained(
                self.model_path, 
                trust_remote_code=True
            )
            
            # è®¾ç½®è‡ªå®šä¹‰çš„max_pixelsä»¥èŠ‚çœæ˜¾å­˜
            if hasattr(self.processor, 'image_processor') and self.max_pixels:
                self.processor.image_processor.max_pixels = self.max_pixels
                logger.info(f"âœ… å·²è®¾ç½® max_pixels = {self.max_pixels} (çº¦{self.max_pixels/1e6:.1f}Måƒç´ )")
                logger.info(f"ğŸ’¡ è¿™å¯ä»¥å‡å°‘æ˜¾å­˜å ç”¨ï¼Œé€‚åˆå¤„ç†å¤æ‚å›¾ç‰‡")
            
            # æ ¹æ®é‡åŒ–æ¨¡å¼åŠ è½½æ¨¡å‹
            if self.quantization == "4bit":
                logger.info("ä½¿ç”¨4-bité‡åŒ–æ¨¡å¼")
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    quantization_config=quantization_config,
                    device_map="auto",
                    trust_remote_code=True
                )
                
            elif self.quantization == "8bit":
                logger.info("ä½¿ç”¨8-bité‡åŒ–æ¨¡å¼")
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    load_in_8bit=True,
                    device_map="auto",
                    trust_remote_code=True
                )
                
            elif self.quantization == "cpu":
                logger.info("ä½¿ç”¨CPUæ¨¡å¼")
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                
            else:
                logger.info("ä½¿ç”¨æ ‡å‡†æ¨¡å¼")
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                    trust_remote_code=True
                )
            
            self.device = self.model.device
            logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ! è®¾å¤‡: {self.device}")
            
            # æ˜¾ç¤ºè®¾å¤‡åˆ†é…ä¿¡æ¯
            if hasattr(self.model, 'hf_device_map'):
                logger.info(f"ğŸ“Š è®¾å¤‡æ˜ å°„: {self.model.hf_device_map}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_response(
        self, 
        prompt: str, 
        image_paths: Optional[List[str]] = None,
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå›å¤ï¼ˆä¸å¸¦å†å²è®°å½•ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            generation_config: ç”Ÿæˆé…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒ…å«ç”Ÿæˆç»“æœçš„å­—å…¸
        """
        return self.generate_response_with_history(
            prompt=prompt,
            image_paths=image_paths,
            history=[],
            generation_config=generation_config
        )
    
    def generate_response_with_history(
        self,
        prompt: str,
        image_paths: Optional[List[str]] = None,
        history: Optional[List[Dict[str, Any]]] = None,
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå›å¤ï¼ˆæ”¯æŒå¯¹è¯å†å²å’Œå¤šå›¾ç‰‡ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            generation_config: ç”Ÿæˆé…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒ…å«ç”Ÿæˆç»“æœçš„å­—å…¸
        """
        if self.model is None or self.processor is None:
            return {
                "success": False,
                "error": "æ¨¡å‹æœªåŠ è½½"
            }
        
        try:
            if history is None:
                history = []
            
            if image_paths is None:
                image_paths = []
            
            logger.info(f"ğŸ¤” ç”Ÿæˆå›å¤: {prompt[:50]}... (å›¾ç‰‡æ•°: {len(image_paths)}, å†å²æ¶ˆæ¯æ•°: {len(history)})")
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å†å²å¯¹è¯
            messages = []
            
            # æ·»åŠ å†å²æ¶ˆæ¯
            for hist in history:
                role = hist.get('role')
                content = hist.get('content')
                
                if role and content:
                    # å†å²æ¶ˆæ¯åªåŒ…å«æ–‡æœ¬ï¼ˆå›¾ç‰‡ä¸é‡å¤å‘é€ï¼‰
                    messages.append({
                        "role": role,
                        "content": [{"type": "text", "text": content}]
                    })
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            current_content = []
            
            # æ·»åŠ å¤šå¼ å›¾ç‰‡
            if image_paths and len(image_paths) > 0:
                for image_path in image_paths:
                    current_content.append({
                        "type": "image",
                        "image": image_path
                    })
                logger.info(f"ğŸ–¼ï¸ åŒ…å«{len(image_paths)}å¼ å›¾ç‰‡")
            
            # å¦‚æœæœ‰å¤šå¼ å›¾ç‰‡ï¼Œå¢å¼ºæç¤ºè¯
            enhanced_prompt = prompt
            if image_paths and len(image_paths) > 1:
                enhanced_prompt = f"æˆ‘ä¸Šä¼ äº†{len(image_paths)}å¼ å›¾ç‰‡ã€‚{prompt}\n\nè¯·ä»”ç»†åˆ†ææ¯ä¸€å¼ å›¾ç‰‡ï¼Œå¯¹æ¯”å®ƒä»¬ä¹‹é—´çš„å·®å¼‚å’Œè”ç³»ï¼Œå¹¶ç»™å‡ºç»¼åˆçš„åˆ†æç»“æœã€‚"
                logger.info(f"ğŸ“ æ£€æµ‹åˆ°å¤šå›¾ç‰‡ï¼Œå·²å¢å¼ºæç¤ºè¯")
            
            current_content.append({"type": "text", "text": enhanced_prompt})
            
            messages.append({
                "role": "user",
                "content": current_content
            })
            
            logger.info(f"ğŸ“ æ¶ˆæ¯æ€»æ•°: {len(messages)}")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # å¤„ç†è§†è§‰ä¿¡æ¯ï¼ˆåªå¤„ç†å½“å‰æ¶ˆæ¯ï¼‰
            image_inputs = None
            video_inputs = None
            if image_paths and len(image_paths) > 0:
                # åªå¤„ç†å½“å‰çš„å›¾ç‰‡æ¶ˆæ¯
                current_messages = [messages[-1]]
                image_inputs, video_inputs = process_vision_info(current_messages)
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to(self.model.device)
            
            # é»˜è®¤ç”Ÿæˆé…ç½®
            default_config = {
                "max_new_tokens": 512,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "repetition_penalty": 1.1
            }
            
            # åˆå¹¶ç”¨æˆ·é…ç½®
            if generation_config:
                default_config.update(generation_config)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    **default_config
                )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            # è§£ç è¾“å‡º
            response = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )[0]
            
            logger.info(f"âœ… ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(response)}")
            
            return {
                "success": True,
                "response": response,
                "has_images": len(image_paths) > 0,
                "image_count": len(image_paths)
            }
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": str(e)
            }
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾å†…å­˜"""
        try:
            if self.model is not None:
                del self.model
                self.model = None
            if self.processor is not None:
                del self.processor
                self.processor = None
            
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            logger.info("âœ… æ¨¡å‹å·²å¸è½½")
            return True
        except Exception as e:
            logger.error(f"âŒ å¸è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def is_loaded(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return self.model is not None and self.processor is not None
    
    def preprocess_image(self, image_path: str, max_size: int = 1024) -> str:
        """
        é¢„å¤„ç†å›¾ç‰‡ï¼šå‹ç¼©åˆ†è¾¨ç‡ä»¥èŠ‚çœæ˜¾å­˜
        
        Args:
            image_path: åŸå§‹å›¾ç‰‡è·¯å¾„
            max_size: æœ€å¤§è¾¹é•¿ï¼ˆåƒç´ ï¼‰
            
        Returns:
            å¤„ç†åçš„å›¾ç‰‡è·¯å¾„
        """
        try:
            with Image.open(image_path) as img:
                # è·å–åŸå§‹å°ºå¯¸
                orig_width, orig_height = img.size
                
                # å¦‚æœå›¾ç‰‡ä¸éœ€è¦å‹ç¼©ï¼Œç›´æ¥è¿”å›
                if max(orig_width, orig_height) <= max_size:
                    logger.info(f"ğŸ“· å›¾ç‰‡å°ºå¯¸åˆé€‚ {orig_width}x{orig_height}ï¼Œæ— éœ€å‹ç¼©")
                    return image_path
                
                # è®¡ç®—æ–°å°ºå¯¸ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
                if orig_width > orig_height:
                    new_width = max_size
                    new_height = int(orig_height * max_size / orig_width)
                else:
                    new_height = max_size
                    new_width = int(orig_width * max_size / orig_height)
                
                # å‹ç¼©å›¾ç‰‡
                img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                
                # ä¿å­˜å‹ç¼©åçš„å›¾ç‰‡
                base, ext = os.path.splitext(image_path)
                compressed_path = f"{base}_compressed{ext}"
                img_resized.save(compressed_path, quality=95)
                
                logger.info(f"ğŸ”„ å›¾ç‰‡å·²å‹ç¼©: {orig_width}x{orig_height} â†’ {new_width}x{new_height}")
                logger.info(f"ğŸ’¾ å‹ç¼©åè·¯å¾„: {compressed_path}")
                
                return compressed_path
                
        except Exception as e:
            logger.error(f"âŒ å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {e}")
            return image_path  # å¤±è´¥æ—¶è¿”å›åŸè·¯å¾„
    
    def clear_cuda_cache(self):
        """æ¸…ç†CUDAç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            gc.collect()
            logger.info("ğŸ§¹ å·²æ¸…ç†CUDAç¼“å­˜")
    
    def generate_response_stream(
        self,
        prompt: str,
        image_paths: Optional[List[str]] = None,
        history: Optional[List[Dict[str, Any]]] = None,
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Generator[str, None, None]:
        """
        ç”Ÿæˆå›å¤ï¼ˆæµå¼è¾“å‡ºï¼Œæ”¯æŒå¯¹è¯å†å²å’Œå¤šå›¾ç‰‡ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            generation_config: ç”Ÿæˆé…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        if self.model is None or self.processor is None:
            yield "[é”™è¯¯] æ¨¡å‹æœªåŠ è½½"
            return
        
        try:
            if history is None:
                history = []
            
            if image_paths is None:
                image_paths = []
            
            logger.info(f"ğŸ¤” æµå¼ç”Ÿæˆå›å¤: {prompt[:50]}... (å›¾ç‰‡æ•°: {len(image_paths)}, å†å²æ¶ˆæ¯æ•°: {len(history)})")
            
            # é¢„å¤„ç†å›¾ç‰‡ï¼ˆå‹ç¼©ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
            processed_image_paths = []
            if image_paths and len(image_paths) > 0:
                logger.info("ğŸ–¼ï¸ å¼€å§‹é¢„å¤„ç†å›¾ç‰‡...")
                for img_path in image_paths:
                    processed_path = self.preprocess_image(img_path, max_size=1024)
                    processed_image_paths.append(processed_path)
            else:
                processed_image_paths = image_paths
            
            # æ¸…ç†CUDAç¼“å­˜
            self.clear_cuda_cache()
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å†å²å¯¹è¯
            messages = []
            
            # æ·»åŠ å†å²æ¶ˆæ¯
            for hist in history:
                role = hist.get('role')
                content = hist.get('content')
                
                if role and content:
                    # å†å²æ¶ˆæ¯åªåŒ…å«æ–‡æœ¬ï¼ˆå›¾ç‰‡ä¸é‡å¤å‘é€ï¼‰
                    messages.append({
                        "role": role,
                        "content": [{"type": "text", "text": content}]
                    })
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            current_content = []
            
            # æ·»åŠ å¤šå¼ å›¾ç‰‡
            if image_paths and len(image_paths) > 0:
                for image_path in image_paths:
                    current_content.append({
                        "type": "image",
                        "image": image_path
                    })
                logger.info(f"ğŸ–¼ï¸ åŒ…å«{len(image_paths)}å¼ å›¾ç‰‡")
            
            # å¦‚æœæœ‰å¤šå¼ å›¾ç‰‡ï¼Œå¢å¼ºæç¤ºè¯
            enhanced_prompt = prompt
            if image_paths and len(image_paths) > 1:
                enhanced_prompt = f"æˆ‘ä¸Šä¼ äº†{len(image_paths)}å¼ å›¾ç‰‡ã€‚{prompt}\n\nè¯·ä»”ç»†åˆ†ææ¯ä¸€å¼ å›¾ç‰‡ï¼Œå¯¹æ¯”å®ƒä»¬ä¹‹é—´çš„å·®å¼‚å’Œè”ç³»ï¼Œå¹¶ç»™å‡ºç»¼åˆçš„åˆ†æç»“æœã€‚"
                logger.info(f"ğŸ“ æ£€æµ‹åˆ°å¤šå›¾ç‰‡ï¼Œå·²å¢å¼ºæç¤ºè¯")
            
            current_content.append({"type": "text", "text": enhanced_prompt})
            
            messages.append({
                "role": "user",
                "content": current_content
            })
            
            logger.info(f"ğŸ“ æ¶ˆæ¯æ€»æ•°: {len(messages)}")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # å¤„ç†è§†è§‰ä¿¡æ¯ï¼ˆåªå¤„ç†å½“å‰æ¶ˆæ¯ï¼‰
            image_inputs = None
            video_inputs = None
            if image_paths and len(image_paths) > 0:
                # åªå¤„ç†å½“å‰çš„å›¾ç‰‡æ¶ˆæ¯
                current_messages = [messages[-1]]
                image_inputs, video_inputs = process_vision_info(current_messages)
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to(self.model.device)
            
            # é»˜è®¤ç”Ÿæˆé…ç½®
            default_config = {
                "max_new_tokens": 512,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "repetition_penalty": 1.1
            }
            
            # åˆå¹¶ç”¨æˆ·é…ç½®
            if generation_config:
                default_config.update(generation_config)
            
            # åˆ›å»ºæµå¼è¾“å‡ºå™¨
            streamer = TextIteratorStreamer(
                self.processor.tokenizer,
                skip_prompt=True,
                skip_special_tokens=True
            )
            
            # æ·»åŠ streameråˆ°ç”Ÿæˆé…ç½®
            generation_kwargs = {
                **inputs,
                **default_config,
                "streamer": streamer
            }
            
            # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­ç”Ÿæˆ
            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()
            
            # æµå¼è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
            for text_chunk in streamer:
                yield text_chunk
            
            thread.join()
            
            logger.info("âœ… æµå¼ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            yield f"[é”™è¯¯] {str(e)}"

