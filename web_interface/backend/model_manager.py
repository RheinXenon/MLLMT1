"""
æ¨¡å‹ç®¡ç†å™¨ - è´Ÿè´£åŠ è½½å’Œç®¡ç†Lingshu-7Bæ¨¡å‹

é‡å¤§æ›´æ–°ï¼šé›†æˆå›¾ç‰‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨
- ä¼˜åŒ–å¤šå›¾ç‰‡åœºæ™¯çš„æ€§èƒ½
- é¿å…é‡å¤ç¼–ç å†å²å›¾ç‰‡
- å¤§å¹…é™ä½æ˜¾å­˜å’Œè®¡ç®—å‹åŠ›
"""

import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig, TextIteratorStreamer
from qwen_vl_utils import process_vision_info
import logging
from typing import Optional, Dict, Any, List, Generator
import gc
from threading import Thread
from PIL import Image
import os

from image_context_manager import ImageContextManager, ImageContextStrategy

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ç±»"""
    
    def __init__(
        self, 
        model_path: str, 
        quantization: str = "4bit", 
        max_pixels: int = 1003520,
        image_context_strategy: str = ImageContextStrategy.CURRENT_WITH_TEXT_HISTORY,
        max_recent_images: int = 2,
        enable_image_summary: bool = True
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            quantization: é‡åŒ–æ¨¡å¼ (4bit, 8bit, standard, cpu)
            max_pixels: æœ€å¤§åƒç´ æ•°ï¼Œé»˜è®¤1003520(çº¦100ä¸‡åƒç´ ï¼Œé€‚åˆ8GBæ˜¾å­˜)
            image_context_strategy: å›¾ç‰‡ä¸Šä¸‹æ–‡ç­–ç•¥
            max_recent_images: ä¿ç•™çš„æœ€è¿‘å›¾ç‰‡æ•°é‡
            enable_image_summary: æ˜¯å¦å¯ç”¨å›¾ç‰‡æ‘˜è¦
        """
        self.model_path = model_path
        self.quantization = quantization
        self.max_pixels = max_pixels
        self.model = None
        self.processor = None
        self.device = None
        
        # åˆå§‹åŒ–å›¾ç‰‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.image_context_manager = ImageContextManager(
            strategy=image_context_strategy,
            max_recent_images=max_recent_images,
            enable_summary=enable_image_summary
        )
        logger.info(f"ğŸ¯ å›¾ç‰‡ä¸Šä¸‹æ–‡ç­–ç•¥: {image_context_strategy}")
        
    def check_gpu(self) -> tuple[bool, float]:
        """æ£€æŸ¥GPUå¯ç”¨æ€§"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"âœ… GPU: {gpu_name}")
            logger.info(f"âœ… æ˜¾å­˜: {gpu_memory:.2f} GB")
            return True, gpu_memory
        else:
            logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            return False, 0
    
    def load_model(self) -> bool:
        """
        åŠ è½½æ¨¡å‹
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ”§ å¼€å§‹åŠ è½½æ¨¡å‹ (é‡åŒ–æ¨¡å¼: {self.quantization})...")
            
            # åŠ è½½å¤„ç†å™¨
            logger.info("ğŸ“– åŠ è½½å¤„ç†å™¨...")
            self.processor = AutoProcessor.from_pretrained(
                self.model_path, 
                trust_remote_code=True
            )
            
            # è®¾ç½®è‡ªå®šä¹‰çš„max_pixelsä»¥èŠ‚çœæ˜¾å­˜
            if hasattr(self.processor, 'image_processor') and self.max_pixels:
                self.processor.image_processor.max_pixels = self.max_pixels
                logger.info(f"âœ… å·²è®¾ç½® max_pixels = {self.max_pixels} (çº¦{self.max_pixels/1e6:.1f}Måƒç´ )")
                logger.info(f"ğŸ’¡ è¿™å¯ä»¥å‡å°‘æ˜¾å­˜å ç”¨ï¼Œé€‚åˆå¤„ç†å¤æ‚å›¾ç‰‡")
            
            # æ ¹æ®é‡åŒ–æ¨¡å¼åŠ è½½æ¨¡å‹
            if self.quantization == "4bit":
                logger.info("ä½¿ç”¨4-bité‡åŒ–æ¨¡å¼")
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    quantization_config=quantization_config,
                    device_map="auto",
                    trust_remote_code=True
                )
                
            elif self.quantization == "8bit":
                logger.info("ä½¿ç”¨8-bité‡åŒ–æ¨¡å¼")
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    load_in_8bit=True,
                    device_map="auto",
                    trust_remote_code=True
                )
                
            elif self.quantization == "cpu":
                logger.info("ä½¿ç”¨CPUæ¨¡å¼")
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                
            else:
                logger.info("ä½¿ç”¨æ ‡å‡†æ¨¡å¼")
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                    trust_remote_code=True
                )
            
            self.device = self.model.device
            logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ! è®¾å¤‡: {self.device}")
            
            # æ˜¾ç¤ºè®¾å¤‡åˆ†é…ä¿¡æ¯
            if hasattr(self.model, 'hf_device_map'):
                logger.info(f"ğŸ“Š è®¾å¤‡æ˜ å°„: {self.model.hf_device_map}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_response(
        self, 
        prompt: str, 
        image_paths: Optional[List[str]] = None,
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå›å¤ï¼ˆä¸å¸¦å†å²è®°å½•ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            generation_config: ç”Ÿæˆé…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒ…å«ç”Ÿæˆç»“æœçš„å­—å…¸
        """
        return self.generate_response_with_history(
            prompt=prompt,
            image_paths=image_paths,
            history=[],
            generation_config=generation_config
        )
    
    def generate_response_with_history(
        self,
        prompt: str,
        image_paths: Optional[List[str]] = None,
        history: Optional[List[Dict[str, Any]]] = None,
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå›å¤ï¼ˆæ”¯æŒå¯¹è¯å†å²å’Œå¤šå›¾ç‰‡ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            generation_config: ç”Ÿæˆé…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒ…å«ç”Ÿæˆç»“æœçš„å­—å…¸ï¼ŒåŒ…å«å‹ç¼©åçš„å›¾ç‰‡è·¯å¾„ç”¨äºæ¸…ç†
        """
        if self.model is None or self.processor is None:
            return {
                "success": False,
                "error": "æ¨¡å‹æœªåŠ è½½"
            }
        
        # è®°å½•å‹ç¼©åçš„å›¾ç‰‡è·¯å¾„ï¼ˆç”¨äºæ¸…ç†ï¼‰
        compressed_paths = []
        
        try:
            if history is None:
                history = []
            
            if image_paths is None:
                image_paths = []
            
            logger.info(f"ğŸ¤” ç”Ÿæˆå›å¤: {prompt[:50]}... (å›¾ç‰‡æ•°: {len(image_paths)}, å†å²æ¶ˆæ¯æ•°: {len(history)})")
            
            # ç»Ÿä¸€é¢„å¤„ç†å½“å‰å›¾ç‰‡ï¼ˆå‹ç¼©ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
            if image_paths and len(image_paths) > 0:
                logger.info("ğŸ–¼ï¸ å¼€å§‹é¢„å¤„ç†å½“å‰å›¾ç‰‡...")
                processed_paths = []
                for img_path in image_paths:
                    processed_path = self.preprocess_image(img_path, max_size=1024)
                    processed_paths.append(processed_path)
                    # å¦‚æœç”Ÿæˆäº†å‹ç¼©æ–‡ä»¶ï¼ˆè·¯å¾„ä¸åŒï¼‰ï¼Œè®°å½•ä¸‹æ¥
                    if processed_path != img_path:
                        compressed_paths.append(processed_path)
                image_paths = processed_paths
                logger.info(f"âœ… å½“å‰å›¾ç‰‡é¢„å¤„ç†å®Œæˆï¼Œç”Ÿæˆäº†{len(compressed_paths)}ä¸ªå‹ç¼©æ–‡ä»¶")
            
            # æ¸…ç†CUDAç¼“å­˜
            self.clear_cuda_cache()
            
            # ğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼šä½¿ç”¨å›¾ç‰‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¤„ç†å†å²å›¾ç‰‡
            # æ ¹æ®ç­–ç•¥å†³å®šå¦‚ä½•å¤„ç†å†å²å›¾ç‰‡ï¼ˆè½¬æ–‡æœ¬æè¿° or ä¿ç•™éƒ¨åˆ†å›¾ç‰‡ï¼‰
            logger.info("=" * 60)
            logger.info("ğŸš€ ä½¿ç”¨æ™ºèƒ½å›¾ç‰‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
            
            messages_from_manager, images_to_encode = self.image_context_manager.process_conversation_images(
                current_image_paths=image_paths,
                history=history,
                processor=self.processor,
                model=self.model
            )
            
            logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡å¤„ç†ç»“æœ:")
            logger.info(f"   â€¢ å†å²æ¶ˆæ¯æ•°: {len(messages_from_manager)}")
            logger.info(f"   â€¢ éœ€ç¼–ç å›¾ç‰‡æ•°: {len(images_to_encode)}")
            logger.info(f"   â€¢ ç­–ç•¥: {self.image_context_manager.strategy}")
            logger.info("=" * 60)
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«å½“å‰å›¾ç‰‡ï¼‰
            current_content = []
            
            # æ·»åŠ å½“å‰å›¾ç‰‡
            current_image_count = len(image_paths)
            if image_paths and len(image_paths) > 0:
                for idx, image_path in enumerate(image_paths):
                    current_content.append({
                        "type": "image",
                        "image": image_path
                    })
                    logger.info(f"ğŸ–¼ï¸ å½“å‰æ¶ˆæ¯å›¾ç‰‡ #{idx+1}: {image_path}")
                logger.info(f"ğŸ“¸ å½“å‰æ¶ˆæ¯åŒ…å« {len(image_paths)} å¼ æ–°å›¾ç‰‡")
            
            # æ„å»ºæç¤ºè¯
            enhanced_prompt = prompt
            if current_image_count > 1:
                enhanced_prompt += "\n\nè¯·ä»”ç»†åˆ†ææ¯ä¸€å¼ å›¾ç‰‡ï¼Œå¯¹æ¯”å®ƒä»¬ä¹‹é—´çš„å·®å¼‚å’Œè”ç³»ï¼Œå¹¶ç»™å‡ºç»¼åˆçš„åˆ†æç»“æœã€‚"
            
            current_content.append({"type": "text", "text": enhanced_prompt})
            
            # æ„å»ºæœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨
            messages = messages_from_manager + [{
                "role": "user",
                "content": current_content
            }]
            
            logger.info(f"ğŸ“ æœ€ç»ˆæ¶ˆæ¯æ€»æ•°: {len(messages)}, éœ€ç¼–ç å›¾ç‰‡æ€»æ•°: {len(images_to_encode)}")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # å¤„ç†è§†è§‰ä¿¡æ¯ï¼ˆå¤„ç†æ‰€æœ‰æ¶ˆæ¯ï¼ŒåŒ…æ‹¬å†å²ä¸­çš„å›¾ç‰‡ï¼‰
            image_inputs = None
            video_inputs = None
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ¶ˆæ¯åŒ…å«å›¾ç‰‡
            has_any_images = any(
                any(item.get('type') == 'image' for item in msg.get('content', []))
                for msg in messages
            )
            if has_any_images:
                # å¤„ç†æ‰€æœ‰æ¶ˆæ¯ä¸­çš„å›¾ç‰‡ï¼ˆåŒ…æ‹¬å†å²æ¶ˆæ¯ï¼‰
                image_inputs, video_inputs = process_vision_info(messages)
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to(self.model.device)
            
            # é»˜è®¤ç”Ÿæˆé…ç½®
            default_config = {
                "max_new_tokens": 512,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "repetition_penalty": 1.1
            }
            
            # åˆå¹¶ç”¨æˆ·é…ç½®
            if generation_config:
                default_config.update(generation_config)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    **default_config
                )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            # è§£ç è¾“å‡º
            response = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )[0]
            
            # ç»Ÿè®¡tokenæ¶ˆè€—
            input_tokens = inputs.input_ids.shape[1]  # è¾“å…¥tokenæ•°é‡
            output_tokens = len(generated_ids_trimmed[0])  # è¾“å‡ºtokenæ•°é‡
            total_tokens = input_tokens + output_tokens  # æ€»tokenæ•°é‡
            
            logger.info(f"âœ… ç”Ÿæˆå®Œæˆï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            logger.info(f"ğŸ“Š Tokenæ¶ˆè€—ç»Ÿè®¡:")
            logger.info(f"   â€¢ è¾“å…¥Token: {input_tokens}")
            logger.info(f"   â€¢ è¾“å‡ºToken: {output_tokens}")
            logger.info(f"   â€¢ æ€»Tokenæ•°: {total_tokens}")
            logger.info(f"=" * 60)
            
            # ğŸ”¥ ä¿®å¤ï¼šä¸ºæ–°ä¸Šä¼ çš„å›¾ç‰‡ç”Ÿæˆæ‘˜è¦ï¼ˆç”¨äºåç»­å¯¹è¯ï¼‰
            if image_paths and len(image_paths) > 0 and self.image_context_manager.enable_summary:
                logger.info("ğŸ” å¼€å§‹ä¸ºæ–°ä¸Šä¼ çš„å›¾ç‰‡ç”Ÿæˆæ‘˜è¦...")
                try:
                    summaries = self.image_context_manager.batch_generate_image_summaries(
                        image_paths=image_paths,
                        processor=self.processor,
                        model=self.model,
                        device=self.device
                    )
                    logger.info(f"âœ… å·²ç”Ÿæˆ {len(summaries)} ä¸ªå›¾ç‰‡æ‘˜è¦ï¼Œå·²ç¼“å­˜ä¾›åç»­å¯¹è¯ä½¿ç”¨")
                except Exception as e:
                    logger.warning(f"âš ï¸ ç”Ÿæˆå›¾ç‰‡æ‘˜è¦å¤±è´¥ï¼ˆä¸å½±å“å½“å‰å¯¹è¯ï¼‰: {e}")
            
            return {
                "success": True,
                "response": response,
                "has_images": len(image_paths) > 0,
                "image_count": len(image_paths),
                "compressed_paths": compressed_paths  # è¿”å›å‹ç¼©æ–‡ä»¶è·¯å¾„ç”¨äºæ¸…ç†
            }
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": str(e)
            }
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾å†…å­˜"""
        try:
            if self.model is not None:
                del self.model
                self.model = None
            if self.processor is not None:
                del self.processor
                self.processor = None
            
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            logger.info("âœ… æ¨¡å‹å·²å¸è½½")
            return True
        except Exception as e:
            logger.error(f"âŒ å¸è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def is_loaded(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return self.model is not None and self.processor is not None
    
    def preprocess_image(self, image_path: str, max_size: int = 1024) -> str:
        """
        é¢„å¤„ç†å›¾ç‰‡ï¼šå‹ç¼©åˆ†è¾¨ç‡ä»¥èŠ‚çœæ˜¾å­˜
        
        Args:
            image_path: åŸå§‹å›¾ç‰‡è·¯å¾„
            max_size: æœ€å¤§è¾¹é•¿ï¼ˆåƒç´ ï¼‰
            
        Returns:
            å¤„ç†åçš„å›¾ç‰‡è·¯å¾„
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                logger.warning(f"âš ï¸ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return image_path
            
            with Image.open(image_path) as img:
                # è·å–å›¾ç‰‡æ ¼å¼
                img_format = img.format or 'JPEG'  # é»˜è®¤ä½¿ç”¨JPEGæ ¼å¼
                
                # è·å–åŸå§‹å°ºå¯¸
                orig_width, orig_height = img.size
                
                # å¦‚æœå›¾ç‰‡ä¸éœ€è¦å‹ç¼©ï¼Œç›´æ¥è¿”å›
                if max(orig_width, orig_height) <= max_size:
                    logger.info(f"ğŸ“· å›¾ç‰‡å°ºå¯¸åˆé€‚ {orig_width}x{orig_height}ï¼Œæ— éœ€å‹ç¼©")
                    return image_path
                
                # è®¡ç®—æ–°å°ºå¯¸ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
                if orig_width > orig_height:
                    new_width = max_size
                    new_height = int(orig_height * max_size / orig_width)
                else:
                    new_height = max_size
                    new_width = int(orig_width * max_size / orig_height)
                
                # å‹ç¼©å›¾ç‰‡
                img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                
                # ä¿å­˜å‹ç¼©åçš„å›¾ç‰‡
                base, ext = os.path.splitext(image_path)
                
                # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œæ ¹æ®å›¾ç‰‡æ ¼å¼æ·»åŠ 
                if not ext:
                    format_ext_map = {
                        'PNG': '.png',
                        'JPEG': '.jpg',
                        'JPG': '.jpg',
                        'GIF': '.gif',
                        'BMP': '.bmp',
                        'WEBP': '.webp'
                    }
                    ext = format_ext_map.get(img_format, '.jpg')  # é»˜è®¤ä½¿ç”¨.jpg
                    logger.info(f"ğŸ” æ£€æµ‹åˆ°æ ¼å¼: {img_format}, æ·»åŠ æ‰©å±•å: {ext}")
                
                compressed_path = f"{base}_compressed{ext}"
                
                # æ ¹æ®æ ¼å¼ä¿å­˜ï¼ŒPNGä¸æ”¯æŒqualityå‚æ•°
                if img_format == 'PNG':
                    img_resized.save(compressed_path, format='PNG', optimize=True)
                else:
                    # JPEGç­‰æ ¼å¼æ”¯æŒqualityå‚æ•°
                    img_resized.save(compressed_path, format=img_format, quality=95)
                
                logger.info(f"ğŸ”„ å›¾ç‰‡å·²å‹ç¼©: {orig_width}x{orig_height} â†’ {new_width}x{new_height}")
                logger.info(f"ğŸ’¾ å‹ç¼©åè·¯å¾„: {compressed_path}")
                
                return compressed_path
                
        except Exception as e:
            logger.error(f"âŒ å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return image_path  # å¤±è´¥æ—¶è¿”å›åŸè·¯å¾„
    
    def clear_cuda_cache(self):
        """æ¸…ç†CUDAç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            gc.collect()
            logger.info("ğŸ§¹ å·²æ¸…ç†CUDAç¼“å­˜")
    
    
    def generate_response_stream(
        self,
        prompt: str,
        image_paths: Optional[List[str]] = None,
        history: Optional[List[Dict[str, Any]]] = None,
        generation_config: Optional[Dict[str, Any]] = None,
        compressed_paths_container: Optional[List[str]] = None
    ) -> Generator[str, None, None]:
        """
        ç”Ÿæˆå›å¤ï¼ˆæµå¼è¾“å‡ºï¼Œæ”¯æŒå¯¹è¯å†å²å’Œå¤šå›¾ç‰‡ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            generation_config: ç”Ÿæˆé…ç½®ï¼ˆå¯é€‰ï¼‰
            compressed_paths_container: ç”¨äºè¿”å›å‹ç¼©æ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨å®¹å™¨ï¼ˆå¯é€‰ï¼‰
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        if self.model is None or self.processor is None:
            yield "[é”™è¯¯] æ¨¡å‹æœªåŠ è½½"
            return
        
        try:
            if history is None:
                history = []
            
            if image_paths is None:
                image_paths = []
            
            logger.info(f"ğŸ¤” æµå¼ç”Ÿæˆå›å¤: {prompt[:50]}... (å›¾ç‰‡æ•°: {len(image_paths)}, å†å²æ¶ˆæ¯æ•°: {len(history)})")
            
            # ç»Ÿä¸€é¢„å¤„ç†å½“å‰å›¾ç‰‡ï¼ˆå‹ç¼©ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
            if image_paths and len(image_paths) > 0:
                logger.info("ğŸ–¼ï¸ [æµå¼] å¼€å§‹é¢„å¤„ç†å½“å‰å›¾ç‰‡...")
                processed_paths = []
                for img_path in image_paths:
                    processed_path = self.preprocess_image(img_path, max_size=1024)
                    processed_paths.append(processed_path)
                    # å¦‚æœç”Ÿæˆäº†å‹ç¼©æ–‡ä»¶ï¼ˆè·¯å¾„ä¸åŒï¼‰ï¼Œè®°å½•ä¸‹æ¥
                    if processed_path != img_path and compressed_paths_container is not None:
                        compressed_paths_container.append(processed_path)
                image_paths = processed_paths
                if compressed_paths_container is not None:
                    logger.info(f"âœ… [æµå¼] å½“å‰å›¾ç‰‡é¢„å¤„ç†å®Œæˆï¼Œç”Ÿæˆäº†{len(compressed_paths_container)}ä¸ªå‹ç¼©æ–‡ä»¶")
            
            # æ¸…ç†CUDAç¼“å­˜
            self.clear_cuda_cache()
            
            # ğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼šä½¿ç”¨å›¾ç‰‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¤„ç†å†å²å›¾ç‰‡
            logger.info("=" * 60)
            logger.info("ğŸš€ [æµå¼] ä½¿ç”¨æ™ºèƒ½å›¾ç‰‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
            
            messages_from_manager, images_to_encode = self.image_context_manager.process_conversation_images(
                current_image_paths=image_paths,
                history=history,
                processor=self.processor,
                model=self.model
            )
            
            logger.info(f"ğŸ“Š [æµå¼] ä¸Šä¸‹æ–‡å¤„ç†ç»“æœ:")
            logger.info(f"   â€¢ å†å²æ¶ˆæ¯æ•°: {len(messages_from_manager)}")
            logger.info(f"   â€¢ éœ€ç¼–ç å›¾ç‰‡æ•°: {len(images_to_encode)}")
            logger.info(f"   â€¢ ç­–ç•¥: {self.image_context_manager.strategy}")
            logger.info("=" * 60)
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«å½“å‰å›¾ç‰‡ï¼‰
            current_content = []
            
            # æ·»åŠ å½“å‰å›¾ç‰‡
            current_image_count = len(image_paths)
            if image_paths and len(image_paths) > 0:
                for idx, image_path in enumerate(image_paths):
                    current_content.append({
                        "type": "image",
                        "image": image_path
                    })
                    logger.info(f"ğŸ–¼ï¸ [æµå¼] å½“å‰æ¶ˆæ¯å›¾ç‰‡ #{idx+1}: {image_path}")
                logger.info(f"ğŸ“¸ [æµå¼] å½“å‰æ¶ˆæ¯åŒ…å« {len(image_paths)} å¼ æ–°å›¾ç‰‡")
            
            # æ„å»ºæç¤ºè¯
            enhanced_prompt = prompt
            if current_image_count > 1:
                enhanced_prompt += "\n\nè¯·ä»”ç»†åˆ†ææ¯ä¸€å¼ å›¾ç‰‡ï¼Œå¯¹æ¯”å®ƒä»¬ä¹‹é—´çš„å·®å¼‚å’Œè”ç³»ï¼Œå¹¶ç»™å‡ºç»¼åˆçš„åˆ†æç»“æœã€‚"
            
            current_content.append({"type": "text", "text": enhanced_prompt})
            
            # æ„å»ºæœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨
            messages = messages_from_manager + [{
                "role": "user",
                "content": current_content
            }]
            
            logger.info(f"ğŸ“ [æµå¼] æœ€ç»ˆæ¶ˆæ¯æ€»æ•°: {len(messages)}, éœ€ç¼–ç å›¾ç‰‡æ€»æ•°: {len(images_to_encode)}")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # å¤„ç†è§†è§‰ä¿¡æ¯ï¼ˆå¤„ç†æ‰€æœ‰æ¶ˆæ¯ï¼ŒåŒ…æ‹¬å†å²ä¸­çš„å›¾ç‰‡ï¼‰
            image_inputs = None
            video_inputs = None
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ¶ˆæ¯åŒ…å«å›¾ç‰‡
            has_any_images = any(
                any(item.get('type') == 'image' for item in msg.get('content', []))
                for msg in messages
            )
            if has_any_images:
                # å¤„ç†æ‰€æœ‰æ¶ˆæ¯ä¸­çš„å›¾ç‰‡ï¼ˆåŒ…æ‹¬å†å²æ¶ˆæ¯ï¼‰
                image_inputs, video_inputs = process_vision_info(messages)
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to(self.model.device)
            
            # é»˜è®¤ç”Ÿæˆé…ç½®
            default_config = {
                "max_new_tokens": 512,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "repetition_penalty": 1.1
            }
            
            # åˆå¹¶ç”¨æˆ·é…ç½®
            if generation_config:
                default_config.update(generation_config)
            
            # åˆ›å»ºæµå¼è¾“å‡ºå™¨
            streamer = TextIteratorStreamer(
                self.processor.tokenizer,
                skip_prompt=True,
                skip_special_tokens=True
            )
            
            # æ·»åŠ streameråˆ°ç”Ÿæˆé…ç½®
            generation_kwargs = {
                **inputs,
                **default_config,
                "streamer": streamer
            }
            
            # ç”¨äºä¿å­˜ç”Ÿæˆçš„token IDs
            generated_ids_container = []
            
            # å®šä¹‰ç”Ÿæˆå‡½æ•°ï¼Œä¿å­˜ç»“æœ
            def generate_with_save():
                result = self.model.generate(**generation_kwargs)
                generated_ids_container.append(result)
            
            # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­ç”Ÿæˆ
            thread = Thread(target=generate_with_save)
            thread.start()
            
            # æµå¼è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
            generated_text = ""
            for text_chunk in streamer:
                generated_text += text_chunk
                yield text_chunk
            
            thread.join()
            
            # ç»Ÿè®¡tokenæ¶ˆè€—
            input_tokens = inputs.input_ids.shape[1]  # è¾“å…¥tokenæ•°é‡
            
            # ä»ä¿å­˜çš„ç»“æœä¸­è·å–è¾“å‡ºtokenæ•°é‡
            if generated_ids_container:
                generated_ids = generated_ids_container[0]
                output_tokens = generated_ids.shape[1] - input_tokens  # è¾“å‡ºtokenæ•°é‡
            else:
                # å¦‚æœæ— æ³•è·å–ç”Ÿæˆçš„IDsï¼Œä½¿ç”¨æ–‡æœ¬é•¿åº¦ä¼°ç®—
                output_tokens = len(generated_text)  # ç²—ç•¥ä¼°è®¡
            
            total_tokens = input_tokens + output_tokens  # æ€»tokenæ•°é‡
            
            logger.info("âœ… æµå¼ç”Ÿæˆå®Œæˆ")
            logger.info(f"ğŸ“Š Tokenæ¶ˆè€—ç»Ÿè®¡:")
            logger.info(f"   â€¢ è¾“å…¥Token: {input_tokens}")
            logger.info(f"   â€¢ è¾“å‡ºToken: {output_tokens}")
            logger.info(f"   â€¢ æ€»Tokenæ•°: {total_tokens}")
            logger.info(f"=" * 60)
            
            # ğŸ”¥ ä¿®å¤ï¼šä¸ºæ–°ä¸Šä¼ çš„å›¾ç‰‡ç”Ÿæˆæ‘˜è¦ï¼ˆç”¨äºåç»­å¯¹è¯ï¼‰
            if image_paths and len(image_paths) > 0 and self.image_context_manager.enable_summary:
                logger.info("ğŸ” [æµå¼] å¼€å§‹ä¸ºæ–°ä¸Šä¼ çš„å›¾ç‰‡ç”Ÿæˆæ‘˜è¦...")
                try:
                    summaries = self.image_context_manager.batch_generate_image_summaries(
                        image_paths=image_paths,
                        processor=self.processor,
                        model=self.model,
                        device=self.device
                    )
                    logger.info(f"âœ… [æµå¼] å·²ç”Ÿæˆ {len(summaries)} ä¸ªå›¾ç‰‡æ‘˜è¦ï¼Œå·²ç¼“å­˜ä¾›åç»­å¯¹è¯ä½¿ç”¨")
                except Exception as e:
                    logger.warning(f"âš ï¸ [æµå¼] ç”Ÿæˆå›¾ç‰‡æ‘˜è¦å¤±è´¥ï¼ˆä¸å½±å“å½“å‰å¯¹è¯ï¼‰: {e}")
            
        except Exception as e:
            logger.error(f"âŒ æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            yield f"[é”™è¯¯] {str(e)}"

